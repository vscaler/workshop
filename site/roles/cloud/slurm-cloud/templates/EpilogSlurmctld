#!/bin/bash

# this cleans up job folders created on the stage-in/out step
# We only make job dirs if the job comes via the datajobs partition so they are forced to use the QoS

if [ "$SLURM_JOB_PARTITION" = "datajobs" ]; then
    # IF the job folder is empty after the stage in job has completed then assume that we are a stage out job. remove it.
    # note that using rmdir means that it will not delete if there is data in there so we do NOT need to test first
    rmdir {{ aws.sharedfs_mountpoint }}/${SLURM_JOB_ID} &> /dev/null

    # get all directories which have correctly completed
    for mypath in $(ls {{ aws.sharedfs_mountpoint }}/*/SLURM_epilogue_postprocess_delete_me_please); do
        #strip off the key filename
        base=${mypath/\/SLURM_epilogue_postprocess_delete_me_please/}

        # count the the entries in the job folder
        filecount=$(ls $base | wc -l)

        # if there is only one then it is safe to delete
        if [ $filecount -eq 1 ]; then
            rm "$mypath"
            rmdir "$base"
        fi
    done
fi
